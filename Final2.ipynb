{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "dir0 = Path('el_nino/')\n",
    "file_sst = 'sst.mnmean.nc'\n",
    "file_2 = 'mslp_coarse.nc'\n",
    "\n",
    "# load the data set with xarray\n",
    "ds_nino = xr.open_dataset(Path(dir0, file_sst))\n",
    "ds_mslp = xr.open_dataset(Path(dir0, file_2))\n",
    "\n",
    "# define 3.4 region\n",
    "lat_min, lat_max = -5.5, 5.5\n",
    "lon_min, lon_max = 190, 240\n",
    "\n",
    "# Interpolating to get rid of the nan-values\n",
    "ds_nino = ds_nino.interpolate_na(dim='lon')\n",
    "ds_mslp = ds_mslp.interpolate_na(dim='lon')\n",
    "\n",
    "# Select the region\n",
    "ds_region_nino = ds_nino.where((ds_nino.lat >= lat_min) & (ds_nino.lat <= lat_max) & \n",
    "                               (ds_nino.lon >= lon_min) & (ds_nino.lon <= lon_max), drop=True)\n",
    "ds_region_mslp = ds_mslp.where((ds_mslp.latitude >= lat_min) & (ds_mslp.latitude <= lat_max) & \n",
    "                               (ds_mslp.longitude >= lon_min) & (ds_mslp.longitude <= lon_max), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the labels from 01/1982 to 05/2021\n",
    "# -2 = Strong La Nina\n",
    "# -1 = La Nina\n",
    "# 0 = Nothing\n",
    "# 1 = El Nino\n",
    "# 2 = Strong El Nino\n",
    "\n",
    "pred_2023 = {}\n",
    "\n",
    "# Initialisation\n",
    "start_date_y = pd.Timestamp(year = 1982, month = 1, day = 1)\n",
    "end_date_y = pd.Timestamp(year = 2021, month = 5, day = 1)\n",
    "current_date = start_date_y\n",
    "\n",
    "# Mean temperature in the region over all the years\n",
    "big_mean = float(ds_region_nino.mean()['sst'])\n",
    "\n",
    "ys = []\n",
    "\n",
    "while current_date <= end_date_y:\n",
    "    # print(current_date)\n",
    "\n",
    "    # Create Timestamps for previous, current, and next months\n",
    "    current_month = current_date\n",
    "    prev_month = current_month - pd.DateOffset(months = 1)\n",
    "    next_month = current_month + pd.DateOffset(months = 1)\n",
    "\n",
    "    # Get data for each month\n",
    "    ds_prev_month = ds_region_nino.sel(time = slice(prev_month, prev_month))\n",
    "    ds_curr_month = ds_region_nino.sel(time = slice(current_month, current_month))\n",
    "    ds_next_month = ds_region_nino.sel(time = slice(next_month, next_month))\n",
    "\n",
    "    # Merge the three datasets\n",
    "    merged_dataset = xr.concat([ds_prev_month, ds_curr_month, ds_next_month], dim='time')\n",
    "\n",
    "    # Calculate the average sea surface temperature along the time dimension\n",
    "    sst_anom = float(merged_dataset['sst'].mean()) - big_mean\n",
    "    # print(current_date, ': ', sst_anom)\n",
    "    cases = [\n",
    "        (sst_anom >= 0.5),\n",
    "        (sst_anom < 0.5) & (sst_anom > -0.5),\n",
    "        (sst_anom <= -0.5),\n",
    "    ]\n",
    "    conditions = [1, 0, -1]\n",
    "    res = np.select(cases, conditions, 0)\n",
    "\n",
    "    ys.append(res)\n",
    "\n",
    "    # Increment to the first day of the next month\n",
    "    current_date += pd.DateOffset(months = 1)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "ys_np = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_month = 1\n",
    "\n",
    "# Dataset to predict n_month in advance using 1 year of data\n",
    "start_date_X = start_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n",
    "end_date_X = end_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n",
    "current_date = start_date_X\n",
    "\n",
    "xs_np = {}\n",
    "xs_np_pred ={}\n",
    "\n",
    "while current_date <= end_date_X:\n",
    "    \n",
    "    start_variable = current_date\n",
    "    end_variable = current_date + pd.DateOffset(years = 1) - pd.DateOffset(months = 1)\n",
    "    # print(start_variable, ' => ', end_variable)\n",
    "\n",
    "    # Selecting the data for the one-year interval\n",
    "    interval_data = ds_mslp.sel(time=slice(start_variable, end_variable))\n",
    "\n",
    "    # Formatting the interval data\n",
    "    numpy_array = interval_data['msl'].to_numpy()\n",
    "    flattened_data = numpy_array.flatten()\n",
    "    xs_np[str(end_variable.year) + \"/\" + str(end_variable.month + n_month)] = flattened_data\n",
    "    # print(str(end_variable.year) + \"/\" + str(end_variable.month + n_month))\n",
    "\n",
    "    # Increment to the first day of the next month\n",
    "    current_date += pd.DateOffset(months = 1)\n",
    "\n",
    "xs_np = np.array(list(xs_np.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473, 781920)\n"
     ]
    }
   ],
   "source": [
    "print(xs_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adete\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, SGD\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[0;32m     12\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(xs_np)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply PCA\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(xs_np)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components =  0.95)  # Keep % of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Splitting the code in Test and Training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, ys_np, test_size = 0.2,shuffle = True, random_state = 42)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 0.5)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes = 3)\n",
    "y_test = to_categorical(y_test, num_classes = 3)\n",
    "\n",
    "# To be used in the neural network\n",
    "rows, cols = X_train.shape\n",
    "print(cols)\n",
    "\n",
    "# L1 Regularization factor\n",
    "l1_lambda = 0.075\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = SGD(learning_rate = 0.2, momentum = 0.4)\n",
    "\n",
    "# Create a Sequential model for regression\n",
    "model = Sequential()\n",
    "\n",
    "# Shape of the neural network\n",
    "model.add(Dense(4, activation = 'relu', input_shape = (cols,), kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(4, activation = 'relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(4, activation = 'relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(3, activation = 'relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(3, activation = 'softmax', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 15, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "\n",
    "print(f\"Train loss: {train_loss}\")\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
