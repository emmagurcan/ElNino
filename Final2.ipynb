{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "dir0 = Path('el_nino/')\n",
    "file_sst = 'sst.mnmean.nc'\n",
    "file_2 = 'mslp_coarse.nc'\n",
    "\n",
    "# load the data set with xarray\n",
    "ds_nino = xr.open_dataset(Path(dir0, file_sst))\n",
    "ds_mslp = xr.open_dataset(Path(dir0, file_2))\n",
    "\n",
    "# define 3.4 region\n",
    "lat_min, lat_max = -5.5, 5.5\n",
    "lon_min, lon_max = 190, 240\n",
    "\n",
    "# Interpolating to get rid of the nan-values\n",
    "ds_nino = ds_nino.interpolate_na(dim='lon')\n",
    "ds_mslp = ds_mslp.interpolate_na(dim='lon')\n",
    "\n",
    "# Select the region\n",
    "ds_region_nino = ds_nino.where((ds_nino.lat >= lat_min) & (ds_nino.lat <= lat_max) & \n",
    "                               (ds_nino.lon >= lon_min) & (ds_nino.lon <= lon_max), drop=True)\n",
    "ds_region_mslp = ds_mslp.where((ds_mslp.latitude >= lat_min) & (ds_mslp.latitude <= lat_max) & \n",
    "                               (ds_mslp.longitude >= lon_min) & (ds_mslp.longitude <= lon_max), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the labels from 01/1982 to 05/2021\n",
    "# -2 = Strong La Nina\n",
    "# -1 = La Nina\n",
    "# 0 = Nothing\n",
    "# 1 = El Nino\n",
    "# 2 = Strong El Nino\n",
    "\n",
    "pred_2023 = {}\n",
    "\n",
    "# Initialisation\n",
    "start_date_y = pd.Timestamp(year = 1982, month = 1, day = 1)\n",
    "end_date_y = pd.Timestamp(year = 2021, month = 5, day = 1)\n",
    "current_date = start_date_y\n",
    "\n",
    "# Mean temperature in the region over all the years\n",
    "big_mean = float(ds_region_nino.mean()['sst'])\n",
    "\n",
    "ys = []\n",
    "\n",
    "while current_date <= end_date_y:\n",
    "    # print(current_date)\n",
    "\n",
    "    # Create Timestamps for previous, current, and next months\n",
    "    current_month = current_date\n",
    "    prev_month = current_month - pd.DateOffset(months = 1)\n",
    "    next_month = current_month + pd.DateOffset(months = 1)\n",
    "\n",
    "    # Get data for each month\n",
    "    ds_prev_month = ds_region_nino.sel(time = slice(prev_month, prev_month))\n",
    "    ds_curr_month = ds_region_nino.sel(time = slice(current_month, current_month))\n",
    "    ds_next_month = ds_region_nino.sel(time = slice(next_month, next_month))\n",
    "\n",
    "    # Merge the three datasets\n",
    "    merged_dataset = xr.concat([ds_prev_month, ds_curr_month, ds_next_month], dim='time')\n",
    "\n",
    "    # Calculate the average sea surface temperature along the time dimension\n",
    "    sst_anom = float(merged_dataset['sst'].mean()) - big_mean\n",
    "    # print(current_date, ': ', sst_anom)\n",
    "    cases = [\n",
    "        (sst_anom >= 0.5),\n",
    "        (sst_anom < 0.5) & (sst_anom > -0.5),\n",
    "        (sst_anom <= -0.5),\n",
    "    ]\n",
    "    conditions = [1, 0, -1]\n",
    "    res = np.select(cases, conditions, 0)\n",
    "\n",
    "    ys.append(res)\n",
    "\n",
    "    # Increment to the first day of the next month\n",
    "    current_date += pd.DateOffset(months = 1)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "ys_np = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_month = 1\n",
    "\n",
    "# Dataset to predict n_month in advance using 1 year of data\n",
    "start_date_X = start_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n",
    "end_date_X = end_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n",
    "current_date = start_date_X\n",
    "\n",
    "xs_np = {}\n",
    "xs_np_pred ={}\n",
    "\n",
    "while current_date <= end_date_X:\n",
    "    \n",
    "    start_variable = current_date\n",
    "    end_variable = current_date + pd.DateOffset(years = 1) - pd.DateOffset(months = 1)\n",
    "    # print(start_variable, ' => ', end_variable)\n",
    "\n",
    "    # Selecting the data for the one-year interval\n",
    "    interval_data = ds_mslp.sel(time=slice(start_variable, end_variable))\n",
    "\n",
    "    # Formatting the interval data\n",
    "    numpy_array = interval_data['msl'].to_numpy()\n",
    "    flattened_data = numpy_array.flatten()\n",
    "    xs_np[str(end_variable.year) + \"/\" + str(end_variable.month + n_month)] = flattened_data\n",
    "    # print(str(end_variable.year) + \"/\" + str(end_variable.month + n_month))\n",
    "\n",
    "    # Increment to the first day of the next month\n",
    "    current_date += pd.DateOffset(months = 1)\n",
    "\n",
    "xs_np = np.array(list(xs_np.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473, 781920)\n"
     ]
    }
   ],
   "source": [
    "print(xs_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\adete\\AppData\\Local\\Temp\\ipykernel_22940\\557627029.py\", line 39, in <module>\n",
      "    model.compile(LinearDiscriminantAnalysis(),\n",
      "  File \"c:\\Users\\adete\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\adete\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Model.compile() got multiple values for argument 'optimizer'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1160, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 283, in executing\n",
      "    assert_(new_stmts <= stmts)\n",
      "  File \"C:\\Users\\adete\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 80, in assert_\n",
      "    raise AssertionError(str(message))\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(xs_np)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components =  0.95)  # Keep % of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Splitting the code in Test and Training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, ys_np, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes = 3)\n",
    "y_test = to_categorical(y_test, num_classes = 3)\n",
    "\n",
    "# To be used in the neural network\n",
    "rows, cols = X_train.shape\n",
    "print(cols)\n",
    "\n",
    "# L1 Regularization factor\n",
    "l1_lambda = 0.05\n",
    "\n",
    "# Create a Sequential model for regression\n",
    "model = Sequential()\n",
    "\n",
    "# Shape of the neural network\n",
    "model.add(Dense(24, activation='relu', input_shape=(cols,), kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(12, activation='relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(3, activation='linear', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 15, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "\n",
    "print(f\"Train loss: {train_loss}\")\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
