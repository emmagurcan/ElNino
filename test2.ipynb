{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "dir0 = Path('el_nino/')\n",
    "file_sst = 'sst.mnmean.nc'\n",
    "file_2 = 'mslp_coarse.nc'\n",
    "\n",
    "# load the data set with xarray\n",
    "ds_nino = xr.open_dataset(Path(dir0, file_sst))\n",
    "ds_mslp = xr.open_dataset(Path(dir0, file_2))\n",
    "\n",
    "# define 3.4 region\n",
    "lat_min, lat_max = -5, 5\n",
    "lon_min, lon_max = 190, 240\n",
    "\n",
    "ds_nino = ds_nino.interpolate_na(dim='lon')\n",
    "ds_mslp = ds_mslp.interpolate_na(dim='lon')\n",
    "\n",
    "# Select the region\n",
    "ds_region_nino = ds_nino.where((ds_nino.lat >= lat_min) & (ds_nino.lat <= lat_max) & \n",
    "                               (ds_nino.lon >= lon_min) & (ds_nino.lon <= lon_max), drop=True)\n",
    "ds_region_mslp = ds_mslp.where((ds_mslp.latitude >= lat_min) & (ds_mslp.latitude <= lat_max) & \n",
    "                               (ds_mslp.longitude >= lon_min) & (ds_mslp.longitude <= lon_max), drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_mslp.head)\n",
    "print(ds_nino.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the labels from 12/1981 to 06/2022\n",
    "# 0 = Nothing\n",
    "# 1 = El Nino\n",
    "# 2 = La Nina\n",
    "\n",
    "\n",
    "# Initialisation\n",
    "start_date_y = pd.Timestamp(year = 1981, month = 12, day = 1)\n",
    "end_date_y = pd.Timestamp(year = 2022, month = 6, day = 1)\n",
    "current_date = start_date_y\n",
    "\n",
    "# Mean temperature in the region over all the years\n",
    "big_mean = float(ds_region_nino.mean()['sst'])\n",
    "\n",
    "ys = []\n",
    "\n",
    "while current_date <= end_date_y:\n",
    "    # print(current_date)\n",
    "\n",
    "    # Create Timestamps for previous, current, and next months\n",
    "    current_month = current_date\n",
    "    prev_month = current_month - pd.DateOffset(months = 1)\n",
    "    next_month = current_month + pd.DateOffset(months = 1)\n",
    "\n",
    "    # Get data for each month\n",
    "    ds_prev_month = ds_region_nino.sel(time = slice(prev_month, prev_month))\n",
    "    ds_curr_month = ds_region_nino.sel(time = slice(current_month, current_month))\n",
    "    ds_next_month = ds_region_nino.sel(time = slice(next_month, next_month))\n",
    "\n",
    "    # Merge the three datasets\n",
    "    merged_dataset = xr.concat([ds_prev_month, ds_curr_month, ds_next_month], dim='time')\n",
    "\n",
    "    # Calculate the average sea surface temperature along the time dimension\n",
    "    sst_anom = float(merged_dataset['sst'].mean()) - big_mean\n",
    "    cases = [\n",
    "        (sst_anom > 1.0),\n",
    "        (sst_anom <= 1.0) & (sst_anom >= -1.0),\n",
    "        (sst_anom < -1.0),\n",
    "    ]\n",
    "    conditions = [1, 0, 2]\n",
    "    res = np.select(cases, conditions, 0)\n",
    "\n",
    "    ys.append(res)\n",
    "    \n",
    "    # Increment to the first day of the next month\n",
    "    current_date += pd.DateOffset(months = 1)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "ys_np = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to predict n_month in advance using 1 year of data\n",
    "\n",
    "# how many month in advance do we want to predict el nino (max: 24 months)\n",
    "n_month = 12\n",
    "\n",
    "start_date_X = start_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n",
    "end_date_X = end_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n",
    "current_date = start_date_X\n",
    "\n",
    "xs_np = {}\n",
    "\n",
    "while current_date <= end_date_X:\n",
    "    \n",
    "    start_variable = current_date\n",
    "    end_variable = current_date + pd.DateOffset(years = 1) - pd.DateOffset(months = 1)\n",
    "    print(start_variable, ' => ', end_variable)\n",
    "\n",
    "    # Selecting the data for the one-year interval\n",
    "    interval_data = ds_mslp.sel(time=slice(start_variable, end_variable))\n",
    "\n",
    "    # Formatting the interval data\n",
    "    numpy_array = interval_data['msl'].to_numpy()\n",
    "    flattened_data = numpy_array.flatten()\n",
    "    xs_np[str(end_variable.year) + \"/\" + str(end_variable.month + n_month)] = flattened_data\n",
    "    # print(str(end_variable.year) + \"/\" + str(end_variable.month + n_month))\n",
    "\n",
    "    # Increment to the next month\n",
    "    current_date += pd.DateOffset(months = 1)\n",
    "\n",
    "xs_np = np.array(list(xs_np.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(487,)\n",
      "(487, 781920)\n"
     ]
    }
   ],
   "source": [
    "print(ys_np.shape)\n",
    "print(xs_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  Predicted\n",
      "0        0          0\n",
      "1        0          0\n",
      "2        0          0\n",
      "3        2          2\n",
      "4        1          1\n",
      "..     ...        ...\n",
      "93       0          0\n",
      "94       2          2\n",
      "95       2          2\n",
      "96       0          0\n",
      "97       0          0\n",
      "\n",
      "[98 rows x 2 columns]\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(xs_np)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_pca, ys_np)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, ys_np, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=3000, random_state=42) \n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(comparison_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy_train}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  Predicted\n",
      "0        1          1\n",
      "1        0          0\n",
      "2        0          0\n",
      "3        2          2\n",
      "4        0          0\n",
      "..     ...        ...\n",
      "93       2          2\n",
      "94       0          0\n",
      "95       0          0\n",
      "96       2          2\n",
      "97       0          0\n",
      "\n",
      "[98 rows x 2 columns]\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(xs_np)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_pca, ys_np)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, ys_np, test_size = 0.2, random_state = 12, shuffle = True)\n",
    "\n",
    "# Create the SVM model with a kernel\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "y_pred_train = svm_model.predict(X_train)\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(comparison_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy_train}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Keep % of variance\n",
    "X_pca = pca.fit_transform(xs_np)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_pca, ys_np)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, ys_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes = 3)\n",
    "y_test = to_categorical(y_test, num_classes = 3)\n",
    "\n",
    "print(X_train.shape)\n",
    "rows, cols = X_train.shape\n",
    "\n",
    "# L1 Regularization factor\n",
    "l1_lambda = 0.1 \n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model with L1 regularization\n",
    "model.add(Dense(6, activation='relu', input_shape=(cols,), kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(12, activation='relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n",
    "model.add(Dense(3, activation='softmax', kernel_regularizer = regularizers.l1(l1_lambda)))  # 3 output units for 3 classes\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 15, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "\n",
    "print(f\"Train loss: {train_loss}\")\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  Predicted\n",
      "0        1          1\n",
      "1        0          0\n",
      "2        0          0\n",
      "3        2          2\n",
      "4        2          2\n",
      "..     ...        ...\n",
      "89       0          0\n",
      "90       2          2\n",
      "91       0          0\n",
      "92       0          0\n",
      "93       0          0\n",
      "\n",
      "[94 rows x 2 columns]\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9893617021276596\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machines\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(xs_np)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_pca, ys_np)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, ys_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class = 3, eval_metric='mlogloss', use_label_encoder = True)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(comparison_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy_train}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.69387755 0.68367347 0.69072165 0.68041237 0.68041237]\n",
      "Average cross-validation score: 0.6858194824321482\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(xs_np)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_pca, ys_np)\n",
    "\n",
    "# Create the SVM model with a kernel\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X_lda, ys_np, cv=5)\n",
    "\n",
    "# cv_scores will hold the score for each fold\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Average cross-validation score:\", cv_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
