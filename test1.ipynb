{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","import xarray as xr\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","import xgboost as xgb\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","dir0 = Path('el_nino/')\n","file_sst = 'sst.mnmean.nc'\n","file_2 = 'mslp_coarse.nc'\n","\n","# Load the data set with xarray\n","ds_nino = xr.open_dataset(Path(dir0, file_sst))\n","ds_mslp = xr.open_dataset(Path(dir0, file_2))\n","\n","# Define 3.4 region\n","lat_min, lat_max = -5.5, 5.5\n","lon_min, lon_max = 190, 240\n","\n","# Interpolating to get rid of the nan-values\n","ds_nino = ds_nino.interpolate_na(dim='lon')\n","ds_mslp = ds_mslp.interpolate_na(dim='lon')\n","\n","# Select the region\n","ds_region_nino = ds_nino.where((ds_nino.lat >= lat_min) & (ds_nino.lat <= lat_max) & \n","                               (ds_nino.lon >= lon_min) & (ds_nino.lon <= lon_max), drop=True)\n","ds_region_mslp = ds_mslp.where((ds_mslp.latitude >= lat_min) & (ds_mslp.latitude <= lat_max) & \n","                               (ds_mslp.longitude >= lon_min) & (ds_mslp.longitude <= lon_max), drop=True)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Extracting the labels from 01/1982 to 05/2021\n","# -1 = La Nina\n","# 0 = Nothing\n","# 1 = El Nino\n","\n","# Initialisation\n","start_date_y = pd.Timestamp(year = 1982, month = 1, day = 1)\n","end_date_y = pd.Timestamp(year = 2021, month = 5, day = 1)\n","current_date = start_date_y\n","\n","# Mean temperature in the region over all the years\n","big_mean = float(ds_region_nino.mean()['sst'])\n","\n","ys = []\n","\n","while current_date <= end_date_y:\n","\n","    # Create timestamps for previous, current, and next months\n","    current_month = current_date\n","    prev_month = current_month - pd.DateOffset(months = 1)\n","    next_month = current_month + pd.DateOffset(months = 1)\n","\n","    # Get data for each month\n","    ds_prev_month = ds_region_nino.sel(time = slice(prev_month, prev_month))\n","    ds_curr_month = ds_region_nino.sel(time = slice(current_month, current_month))\n","    ds_next_month = ds_region_nino.sel(time = slice(next_month, next_month))\n","\n","    # Merge the three datasets\n","    merged_dataset = xr.concat([ds_prev_month, ds_curr_month, ds_next_month], dim='time')\n","\n","    # Calculate the average sea surface temperature anomaly\n","    sst_anom = float(merged_dataset['sst'].mean()) - big_mean\n","    # print(current_date, ': ', sst_anom)\n","\n","    cases = [\n","        (sst_anom >= 0.5),\n","        (sst_anom < 0.5) & (sst_anom > -0.5),\n","        (sst_anom <= -0.5)\n","    ]\n","    conditions = [1, 0, -1]\n","    res = np.select(cases, conditions, 0)\n","\n","    ys.append(res)\n","    \n","    # Increment to the first day of the next month\n","    current_date += pd.DateOffset(months = 1)\n","\n","# Convert the list to a numpy array\n","ys_np = np.array(ys)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# How many month in advance do we want to make predictions\n","n_month = 8\n","\n","# Dataset to predict n_month in advance using 1 year of data\n","start_date_X = start_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n","end_date_X = end_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n","current_date = start_date_X\n","\n","xs_np = {}\n","\n","while current_date <= end_date_X:\n","    \n","    start_variable = current_date\n","    end_variable = current_date + pd.DateOffset(years = 1) - pd.DateOffset(months = 1)\n","    # print(start_variable, ' => ', end_variable)\n","\n","    # Selecting the data for the one-year interval\n","    interval_data = ds_mslp.sel(time = slice(start_variable, end_variable))\n","\n","    # Formatting the interval data\n","    numpy_array = interval_data['msl'].to_numpy()\n","    flattened_data = numpy_array.flatten()\n","    xs_np[str(end_variable.year) + \"/\" + str(end_variable.month + n_month)] = flattened_data\n","\n","    # Increment to the first day of the next month\n","    current_date += pd.DateOffset(months = 1)\n","\n","xs_np = np.array(list(xs_np.values()))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    Actual  Predicted\n","0        0          0\n","1        0          0\n","2       -1         -1\n","3        0          0\n","4        1          1\n","..     ...        ...\n","90      -1         -1\n","91       0          0\n","92       1          1\n","93       0          0\n","94      -1         -1\n","\n","[95 rows x 2 columns]\n","Train Accuracy: 0.8544973544973545\n","Test Accuracy: 0.7684210526315789\n"]}],"source":["# First model :\n","\n","X_train, X_test, y_train, y_test = train_test_split(xs_np, ys_np, test_size = 0.2, random_state = 42)\n","\n","# Create a pipeline with preprocessing and SVM\n","model_1 = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('pca', PCA(n_components = 0.80)),\n","    ('lda', LinearDiscriminantAnalysis()),\n","    ('svm', SVC(kernel='poly'))\n","])\n","\n","# Train the model\n","model_1.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_test = model_1.predict(X_test)\n","y_pred_train = model_1.predict(X_train)\n","\n","# Create a DataFrame for comparison\n","comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test})\n","\n","# Display the DataFrame\n","print(comparison_df)\n","\n","# Evaluate the model\n","accuracy_test = accuracy_score(y_test, y_pred_test)\n","accuracy_train = accuracy_score(y_train, y_pred_train)\n","\n","print(f\"Train Accuracy: {accuracy_train}\")\n","print(f\"Test Accuracy: {accuracy_test}\")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross-validation scores: [0.71578947 0.50526316 0.56842105 0.61702128 0.64893617]\n","Average score: 0.6110862262038074\n"]}],"source":["# Second Model :\n","\n","model_2 = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('pca', PCA(n_components = 0.90)), \n","    ('lda', LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 0.5)),\n","    ('svm', SVC(kernel = 'rbf', C = 0.1, gamma = 0.2))\n","])\n","\n","# Stratified K-Fold Cross-Validation\n","kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n","\n","# Cross-validation scores\n","scores = cross_val_score(model_2, xs_np, ys_np, cv = kfold)\n","\n","# Fit the model on the entire dataset\n","model_2.fit(xs_np, ys_np)\n","\n","print(\"Cross-validation scores:\", scores)\n","print(\"Average score:\", np.mean(scores))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[-1 -1 -1  0  0  0  0 -1 -1 -1 -1 -1]\n"]}],"source":["# Prediction using the model\n","\n","# Which time intervall do we want to predict\n","pred_start_date_y = pd.Timestamp(year = 2022, month = 1, day = 1)\n","pred_end_date_y = pd.Timestamp(year = 2022, month = 12, day = 1)\n","\n","# Dataset to make the prediction\n","pred_start_date_X = pred_start_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n","pred_end_date_X = pred_end_date_y - pd.DateOffset(years = 1) - pd.DateOffset(months = n_month - 1)\n","current_date_pred = pred_start_date_X\n","\n","xs_np_pred = {}\n","\n","# Extracting the data we will use for the prediction\n","while current_date_pred <= pred_end_date_X:\n","\n","    start_variable = current_date_pred\n","    end_variable = current_date_pred + pd.DateOffset(years = 1) - pd.DateOffset(months = 1)\n","    # print(start_variable, ' => ', end_variable)\n","\n","    # Selecting the data for the one-year interval\n","    interval_data = ds_mslp.sel(time=slice(start_variable, end_variable))\n","\n","    # Formatting the interval data\n","    numpy_array = interval_data['msl'].to_numpy()\n","    flattened_data = numpy_array.flatten()\n","    xs_np_pred[str(end_variable.year) + \"/\" + str(end_variable.month + n_month)] = flattened_data\n","\n","    # Going to the next month\n","    current_date_pred += pd.DateOffset(months = 1)\n","\n","xs_np_pred = np.array(list(xs_np_pred.values()))\n","\n","pred_2022 = model_2.predict(xs_np_pred)\n","\n","print(pred_2022)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n","Epoch 1/15\n","12/12 [==============================] - 1s 21ms/step - loss: 1.1072 - accuracy: 0.6243 - val_loss: 1.0211 - val_accuracy: 0.6421\n","Epoch 2/15\n","12/12 [==============================] - 0s 4ms/step - loss: 0.7118 - accuracy: 0.8677 - val_loss: 0.9126 - val_accuracy: 0.6842\n","Epoch 3/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5471 - accuracy: 0.9180 - val_loss: 0.8888 - val_accuracy: 0.7368\n","Epoch 4/15\n","12/12 [==============================] - 0s 4ms/step - loss: 0.4809 - accuracy: 0.9286 - val_loss: 0.8738 - val_accuracy: 0.7263\n","Epoch 5/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4387 - accuracy: 0.9392 - val_loss: 0.9097 - val_accuracy: 0.7263\n","Epoch 6/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4189 - accuracy: 0.9339 - val_loss: 0.9125 - val_accuracy: 0.7368\n","Epoch 7/15\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3968 - accuracy: 0.9339 - val_loss: 0.8745 - val_accuracy: 0.7789\n","Epoch 8/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.9312 - val_loss: 0.8839 - val_accuracy: 0.7684\n","Epoch 9/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.9180 - val_loss: 0.9587 - val_accuracy: 0.7684\n","Epoch 10/15\n","12/12 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.9259 - val_loss: 0.9025 - val_accuracy: 0.7474\n","Epoch 11/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.9259 - val_loss: 0.9530 - val_accuracy: 0.7158\n","Epoch 12/15\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3509 - accuracy: 0.9206 - val_loss: 0.9560 - val_accuracy: 0.7263\n","Epoch 13/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.9365 - val_loss: 0.9090 - val_accuracy: 0.7684\n","Epoch 14/15\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3429 - accuracy: 0.9312 - val_loss: 0.8912 - val_accuracy: 0.7053\n","Epoch 15/15\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.9286 - val_loss: 0.9105 - val_accuracy: 0.7474\n","3/3 [==============================] - 0s 0s/step - loss: 0.9105 - accuracy: 0.7474\n","12/12 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.9365\n","Train loss: 0.32180875539779663\n","Test loss: 0.9105097055435181\n","Train Accuracy: 0.9365079402923584\n","Test Accuracy: 0.7473683953285217\n"]}],"source":["# Neural Network\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam, SGD\n","\n","# Scale the data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(xs_np)\n","\n","# Apply PCA\n","pca = PCA(n_components =  0.95)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Splitting the code in Test and Training set\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, ys_np, test_size = 0.2,shuffle = True, random_state = 42)\n","\n","# Apply LDA\n","lda = LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 0.5)\n","X_train = lda.fit_transform(X_train, y_train)\n","X_test = lda.transform(X_test)\n","\n","# Encode the labels\n","y_train = to_categorical(y_train, num_classes = 3)\n","y_test = to_categorical(y_test, num_classes = 3)\n","\n","# To be used for the first Layer of the neural network\n","rows, cols = X_train.shape\n","print(cols)\n","\n","# L1 Regularization factor\n","l1_lambda = 0.01\n","\n","# Create an optimizer\n","optimizer = SGD(learning_rate = 0.2, momentum = 0.4)\n","\n","# Create a Sequential model for regression\n","model_nn = Sequential()\n","\n","# Shape of the neural network\n","model_nn.add(Dense(4, activation = 'relu', input_shape = (cols,), kernel_regularizer = regularizers.l1(l1_lambda)))\n","model_nn.add(Dense(4, activation = 'relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n","model_nn.add(Dense(4, activation = 'relu', kernel_regularizer = regularizers.l1(l1_lambda)))\n","model_nn.add(Dense(3, activation = 'softmax', kernel_regularizer = regularizers.l1(l1_lambda)))\n","\n","model_nn.compile(optimizer = optimizer, \n","              loss = 'categorical_crossentropy',\n","              metrics = ['accuracy'])\n","\n","history = model_nn.fit(X_train, y_train, epochs = 15, validation_data=(X_test, y_test))\n","\n","# Evaluate the model_nn\n","test_loss, test_accuracy = model_nn.evaluate(X_test, y_test)\n","train_loss, train_accuracy = model_nn.evaluate(X_train, y_train)\n","\n","print(f\"Train loss: {train_loss}\")\n","print(f\"Test loss: {test_loss}\")\n","print(f\"Train Accuracy: {train_accuracy}\")\n","print(f\"Test Accuracy: {test_accuracy}\")"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
